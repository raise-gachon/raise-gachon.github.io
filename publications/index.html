<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | RAISE Lab </title> <meta name="author" content="Jaehyeon Kang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="raise-lab, robotics, artificial intelligence, spatial estimation, slam, world model, gachon university, korea"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://raise-gachon.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> RAISE Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Teaching </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/reading/">Reading Group</a> <a class="dropdown-item " href="/study/">Study Group</a> <a class="dropdown-item " href="/courses/">Courses</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <div class="scholar-link mb-4"> <p>For a complete and up-to-date list of publications, please visit <a href="https://scholar.google.com/citations?user=KCgEeDoAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer" style="font-family: 'Courier New', Courier, monospace;">[Google Scholar Profile]</a> . </p> </div> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/TIM25_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="TIM25_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TIM25" class="col-sm-8"> <div class="title">Retraining-Free Camera Localization in Indoor Point Clouds Using Edges and Normals</div> <div class="author"> <em>Jaehyeon Kang</em> </div> <div class="periodical"> <em>IEEE Transactions on Instrumentation and Measurement (TIM)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/TIM.2025.3597621" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Estimating a 6-DoF camera pose in a prebuilt map is essential for spatial perception in robotics and autonomous driving. While structure-from-motion (SfM) maps provide visual context, they lack geometric precision and scale. In contrast, point cloud maps from LiDAR or depth cameras offer accurate geometry for metric-level perception. This article presents a novel camera localization method that enhances pose estimation accuracy in indoor point cloud maps that do not contain visual information. By leveraging low-level features such as edges and normals from both the query image and the point cloud, the proposed approach operates across diverse environments without scene-specific training. Specifically, we introduce an edge alignment cost that measures the edge point reprojection errors and a normal distribution cost using normalized information distance (NID) to quantify normal vector similarity. An adaptive weighting scheme integrates these costs in an optimization framework. Experiments on simulated and real-world datasets demonstrate that our method outperforms state-of-the-art feature-matching algorithms in localization accuracy.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/TII24_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="TII24_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TII24" class="col-sm-8"> <div class="title">A Measure of Semantic Class Difference of Point Reprojection Pairs in Camera Pose Estimation</div> <div class="author"> <em>Jaehyeon Kang</em> and Changjoo Nam </div> <div class="periodical"> <em>IEEE Transactions on Industrial Informatics (TII)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/TII.2023.3258443" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In this article, we propose a new measure that evaluates the semantic errors of camera poses in visual odometry (VO) and visual simultaneous localization and mapping (VSLAM). Traditionally, VO/VSLAM methods have used photometric images to estimate camera poses, but they suffer from varying illumination and viewpoint changes. Thus, methods using semantic images have been an alternative to increase consistency, as semantic information has shown its robustness even in hostile environments. Our measure compares semantic classes of map point reprojection pairs between images to improve the camera pose estimation accuracy in VO/VSLAM. To evaluate the difference between semantic classes, we adopt the normalized information distance from information theory. Furthermore, we suggest a weight parameter to balance the existing error of VO/VSLAM with the semantic error introduced by our approach. Our experimental results, obtained from the VKITTI and KITTI benchmark datasets, show that the proposed semantic error measure reduces both the relative pose error and absolute trajectory error of camera pose estimation compared to the existing photometric image-based errors of indirect and direct VO/VSLAM.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/EAAI22_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="EAAI22_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="EAAI22" class="col-sm-8"> <div class="title">Spatial template-based geometric complexity reduction method for photo-realistic modeling of large-scale indoor spaces</div> <div class="author"> Janghun Hyeon, Joohyung Kim, Hyunga Choi, Bumchul Jang, <em>Jaehyeon Kang</em>, and Nakju Doh </div> <div class="periodical"> <em>Engineering Applications of Artificial Intelligence</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.engappai.2022.105369" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Recent progresses in image-based rendering (IBR) have demonstrated the feasibility of photo-realistic modeling in room-scale indoor spaces. However, it is difficult to extend the method to large-scale indoor spaces, because the computational complexity increases exponentially as the geometric complexity increases. In this study, we propose a framework that automatically generates photo-realistic model of large-scale indoor spaces. We first define primary factors that increase geometrical complexity as geometrically excluded objects (GEOs). The proposed framework removes GEOs in images and point clouds to efficiently represent large-scale indoor spaces. To this end, we introduce a segmentation method to segment GEOs from every image coherently. In addition, we also introduce an image inpainting method to fill in the segmented images for photo-realistic indoor modeling. Experiments are conducted in three small-scale spaces and two large-scale indoor spaces. In the experiments, the proposed modules are validated thoroughly. In addition, the experimental results show that the proposed method enables to generate photo-realistic indoor models automatically and efficiently.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/CEA22_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CEA22_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="CEA22" class="col-sm-8"> <div class="title">Tomato harvesting robotic system based on Deep-ToMaToS: Deep learning network using transformation loss for 6D pose estimation of maturity classified tomatoes with side-stem</div> <div class="author"> JoonYoung Kim, HyeRan Pyo, Inhoon Jang, <em>Jaehyeon Kang</em>, ByeongKwon Ju, and KwangEun Ko </div> <div class="periodical"> <em>Computers and Electronics in Agriculture</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.compag.2022.107300" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper presents the development of an autonomous harvesting robot system for tomato, a representative crop cultivated in the facility horticulture smart farm. The automated harvesting work using a robotic system is very challenging because of the appearance, environmental features, such as the atypical directions of the peduncles or their growing form in a bunch. Also, the robot system should enable the harvesting of the target fruit only without damaging other fruits, stems, and branches. Hence, this paper presents a deep learning network pipeline, Deep-ToMaToS, capable of three-level maturity classification and 6D pose (3D translation + 3D rotation) estimation of the target fruit simultaneously. Due to the difficulties encountered in building a large-scale dataset to train and test the deep learning model for the 6D pose estimation in the real world, we presented an automatic data collection scheme based on a photo-realistic 3D simulator environment. The robotic harvesting system includes a harvesting motion control algorithm based on the result of the 6D pose estimation. The overall process of the motion control phase is described along with the decision way of the appropriate final posture of the harvesting module mounted at the end-effector of the robot manipulator via removal of invalid motions getting out of the valid workspace or redundant motions. We conducted experiments on the 6D pose estimation based on the Deep-ToMaToS and the harvesting motion control in virtual and real smart farm environments. The experimental results showed a 6D pose estimation accuracy of 96 % based on the ADD_S metric, and the proposed harvesting motion control algorithm achieves the harvesting success rate of 84.5 % on average. The experimental results reveal that the harvesting robot system has significant potential to extend to harvesting works for other fruits and crops.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/TIM22_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="TIM22_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TIM22" class="col-sm-8"> <div class="title">Precise Camera–LiDAR Extrinsic Calibration Based on a Weighting Strategy Using Analytic Plane Covariances</div> <div class="author"> Gunhee Koo, <em>Jaehyeon Kang</em>, Bumchul Jang, and Nakju Doh </div> <div class="periodical"> <em>IEEE Transactions on Instrumentation and Measurement (TIM)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/TIM.2022.3202549" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Planarity is a practical feature for estimating the camera–LiDAR extrinsic parameters. LiDAR directly measures the 3-D plane in the form of a point cloud, but the camera captures the 2-D plane image which requires perspective-n-point (PnP) techniques for 3-D plane estimation. Likewise, these 3-D plane acquisition processes by LiDAR and camera are quite different; hence, their plane covariances from each sensor also differ. The plane measurement covariance originates from the LiDAR measurement noise; it can be directly calculated from a 3-D point cloud. The plane parameter covariances originate from the corner point errors in a 2-D image; however, their direct formulas have not been studied. In this study, we analytically derive the plane parameter covariances under the assumption of linear error propagation. To validate the proposed covariances, we implemented a simulation to observe how effectively the proposed covariances reflect on the true plane parameter covariances. Next, we validated that the proposed covariances could increase the performance of the camera–LiDAR extrinsic calibration. We compared our method with other calibration methods by varying the pixel noise levels and the number of board measurements in calibration simulations. In calibration field tests, we calibrated a scanning system equipped with a Ladybug5+ camera and a VLP-16 LiDAR and compared our method with other calibration methods.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/ICRA20_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ICRA20_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ICRA20" class="col-sm-8"> <div class="title">Analytic Plane Covariances Construction for Precise Planarity-based Extrinsic Calibration of Camera and LiDAR</div> <div class="author"> Gunhee Koo, <em>Jaehyeon Kang</em>, Bumchul Jang, and Nakju Doh </div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/ICRA40945.2020.9197149" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>IPlanarity of checkerboards is a widely used feature for extrinsic calibration of camera and LiDAR. In this study, we propose two analytically derived covariances of (i) plane parameters and (ii) plane measurement, for precise extrinsic calibration of camera and LiDAR. These covariances allow the graded approach in planar feature correspondences by exploiting the uncertainty of a set of given features in calibration. To construct plane parameter covariance, we employ the error model of 3D corner points and the analytically formulated plane parameter errors. Next, plane measurement covariance is directly derived from planar regions of point clouds using the out-of-plane errors. In simulation validation, our method is compared to an existing uncertainty-excluding method using the different number of target poses and the different levels of noise. In field experiment, we validated the applicability of the proposed analytic plane covariances for precise calibration using the basic planarity-based method and the latest planarity-and-linearity-based method.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/RAL20_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="RAL20_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="RAL20" class="col-sm-8"> <div class="title">Modeling of architectural components for large-scale indoor spaces from point cloud measurements</div> <div class="author"> Gahyeon Lim, Youjin Oh, Dongwoo Kim, ChangHyun Jun, <em>Jaehyeon Kang</em>, and Nakju Doh </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/LRA.2020.2976327" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In this letter, we propose a method to model architectural components in large-scale indoor spaces from point cloud measurements. The proposed method enables the modeling of curved surfaces, cylindrical pillars, and slanted surfaces, which cannot be modeled using existing approaches. It operates by constructing the architectural points from the raw point cloud after removing non-architectural (objects) points and filling in the holes caused by their exclusion. Then, the architectural points are represented using a set of piece-wise planar segments. Finally, the adjacency graph of the planar segments is constructed to verify the fact that every planar segment is closed. This ensures a watertight mesh model generation. Experimentation using 14 different real-world indoor space datasets and 2 public datasets, comprising spaces of various sizes-from room-scale to large-scale (12,557 m2), verify the accuracy of the proposed method in modeling environments with curved surfaces, cylindrical pillars, and slanted surfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/JFR20_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="JFR20_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="JFR20" class="col-sm-8"> <div class="title">Automatic targetless camera–LIDAR calibration by aligning edge with Gaussian mixture model</div> <div class="author"> <em>Jaehyeon Kang</em> and Nakju Doh </div> <div class="periodical"> <em>Journal of Field Robotics (JFR)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1002/rob.21893" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper presents a calibration algorithm that does not require an artificial target object to precisely estimate a rigid-body transformation between a camera and a light detection and ranging (LIDAR) sensor. The proposed algorithm estimates calibration parameters by minimizing a cost function that evaluates the edge alignment between two sensor measurements. In particular, the proposed cost function is constructed using a projection model-based many-to-many correspondence of the edges to fully exploit measurements with different densities (dense photometry and sparse geometry). The alignment of the many-to-many correspondence is represented using the Gaussian mixture model (GMM) framework. Here, each component of the GMM, including weight, displacement, and standard deviation, is derived to suitably capture the intensity, location, and influential range of the edge measurements, respectively. The derived cost function is optimized by the gradient descent method with an analytical derivative. A coarse-to-fine scheme is also applied by gradually decreasing the standard deviation of the GMM to enhance the robustness of the algorithm. Extensive indoor and outdoor experiments validate the claim that the proposed GMM strategy improves the performance of the proposed algorithm. The experimental results also show that the proposed algorithm outperforms previous methods in terms of precision and accuracy by providing calibration parameters of standard deviations less than 0.6° and 2.1 cm with a reprojection error of 1.78 for a 2.1-megapixel image (2,048 × 1,024) in the best case.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/IROS19_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IROS19_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IROS19" class="col-sm-8"> <div class="title">Automatic spatial template generation for realistic 3D modeling of large-scale indoor spaces</div> <div class="author"> Janghun Hyeon, Hyunga Choi, JooHyung Kim, Bumchul Jang, <em>Jaehyeon Kang</em>, and Nakju Doh </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/IROS40897.2019.8968286" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper proposes a realistic indoor modeling framework for large-scale indoor spaces. The proposed framework reduces the geometric complexity of an indoor model to efficiently represent large-scale environments for image-based rendering (IBR) approaches. For this purpose, the proposed framework removes geometrically excluded objects (GEOs) in point cloud and images, which represent the primary factors in high geometric complexity. In particular, GEOs are coherently removed from all images using a global geometry model. Then, the remaining holes are inpainted using globally consistent guidelines, to achieve accurate image blending in IBR approaches. The experimental results verify that the proposed GEO removal framework provides efficient point clouds and images for realistic indoor modeling in large-scale indoor spaces.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/IASC17_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IASC17_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IASC17" class="col-sm-8"> <div class="title">Towards a realistic indoor world reconstruction: Preliminary results for an object-oriented 3D RGB-D mapping</div> <div class="author"> ChangHyun Jun, <em>Jaehyeon Kang</em>, Suyong Yeon, Hyunga Choi, Tae-Young Chung, and Nakju Doh </div> <div class="periodical"> <em>Intelligent Automation &amp; Soft Computing</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1080/10798587.2016.1186890" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>A real world reconstruction that generates cyberspace not from a computer graphics tool, but from the real world, has been one of the main issues in two different communities of robotics and computer vision under different names of Simultaneous Localization And Mapping (SLAM) and Structure from Motion (SfM). However, there have been few trials that actively integrate SLAM and SfM for possible synergy. This paper shows the real world reconstruction can be enabled through this integration. As a result, the preliminary map has been generated of which five subgoals are: Realistic view (RGB), accurate geometry (depth), applicability to multi-floor indoor building, initial classification of a possible set of objects, and full automation. To this end, an engineering framework of “Acquire-Build-Comprehend (ABC)” is proposed, through which a sensor system acquires an RGB-Depth point cloud from the real world, builds a three-dimensional map, and comprehends this map to yield the possible set of objects. Its performance is demonstrated by building a map for three levels of indoor building of which volume is 1,408 m3.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/TRO16_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="TRO16_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TRO16" class="col-sm-8"> <div class="title">Full-DOF calibration of a rotating 2-D LIDAR with a simple plane measurement</div> <div class="author"> <em>Jaehyeon Kang</em> and Nakju Doh </div> <div class="periodical"> <em>IEEE Transactions on Robotics (T-RO)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/TRO.2016.2596769" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper proposes a calibration method that accurately estimates six parameters between the two centers of 2-D light detection and ranging (LIDAR) and a rotating platform. This method uses a simple plane, and to the best of our knowledge, it is the first to enable full-degree-of-freedom (DOF) estimation without additional hardware. The key concept behind this method is a decoupling property, in which the direction of a line on a plane does not contain 3-DOF translation terms. Based on this, a cost function for rotation is constructed, and 3-DOF rotation parameters are estimated. With this rotation, the remaining 3-DOF translation parameters are calculated in a manner that minimizes the cost function for translation only. In other words, an original 6-DOF problem is decoupled into two 3-DOF estimation problems. Given these cost functions, degenerate cases are mathematically analyzed for known cases (incomplete), and the robustness is numerically tested for all possible cases (complete). The performance of the method is validated by extensive simulations and experimentations, and the estimated parameters from the proposed method demonstrate better accuracy than previous methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/RAL16_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="RAL16_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="RAL16" class="col-sm-8"> <div class="title">Accurate Continuous Sweeping Framework in Indoor Spaces With Backpack Sensor System for Applications to 3-D Mapping</div> <div class="author"> Keonyong Lee, Soo-Hyun Ryu, Suyong Yeon, HyunGi Cho, ChangHyun Jun, <em>Jaehyeon Kang</em>, Hyunga Choi, Janghun Hyeon, Insik Baek, Woonhyung Jung, Hanul Kim, and Nakju Doh </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/LRA.2016.2516585" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In indoor environments, there exists a few distinctive indoor spaces’ features (ISFs). However, up to our knowledge, there is no algorithm that fully utilizes ISF for accurate 3-D SLAM. In this letter, we suggest a sensor system that efficiently captures ISF and propose an algorithm framework that accurately estimates sensor’s 3-D poses by utilizing ISF. Experiments conducted in six representative indoor spaces show that the accuracy of the proposed method is better than the previous method. Furthermore, the proposed method shows robust performances in a sense that a set of adjusted parameters of the related algorithms does not need to be recalibrated as target environment changes. We also demonstrate that the proposed method not only generates 3-D depth maps but also builds a dense 3-D RGB-D map.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/URAI14_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="URAI14_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="UR14" class="col-sm-8"> <div class="title">Analysis of the reference coordinate system used in the EKF-based SLAM</div> <div class="author"> Keonyong Lee, Soo-hyun Ryu, <em>Jaehyeon Kang</em>, Hyunga Choi, ChangHyun Jun, and Nakju Doh </div> <div class="periodical"> <em>In International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1109/URAI.2014.7057516" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In this paper we analyze the differences in the EKF-SLAM frameworks that are constructed using different coordinate system. First, we derive the EKF equations in the world coordinate system through modification of the EKF equations in the robot coordinate system. When the reference coordinate system is changed, the equations change due to its nonlinear behavior. We identify four EKF-SLAM frameworks, and then we demonstrate that any framework can be used in EKF-SLAM by simulation and real-life experiments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/IR14_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="IR14_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="IR14" class="col-sm-8"> <div class="title">Robust-PCA-based hierarchical plane extraction for application to geometric 3D indoor mapping</div> <div class="author"> Suyong Yeon, ChangHyun Jun, Hyunga Choi, <em>Jaehyeon Kang</em>, Youngmok Yun, and Nakju Doh </div> <div class="periodical"> <em>Industrial Robot: An International Journal</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1108/IR-04-2013-347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Purpose - The authors aim to propose a novel plane extraction algorithm for geometric 3D indoor mapping with range scan data. Design/methodology/approach - The proposed method utilizes a divide-and-conquer step to efficiently handle huge amounts of point clouds not in a whole group, but in forms of separate sub-groups with similar plane parameters. This method adopts robust principal component analysis to enhance estimation accuracy. Findings - Experimental results verify that the method not only shows enhanced performance in the plane extraction, but also broadens the domain of interest of the plane registration to an information-poor environment (such as simple indoor corridors), while the previous method only adequately works in an information-rich environment (such as a space with many features). Originality/value - The proposed algorithm has three advantages over the current state-of-the-art method in that it is fast, utilizes more inlier sensor data that does not become contaminated by severe sensor noise and extracts more accurate plane parameters.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/RRT14_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="RRT14_preview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="RRT14" class="col-sm-8"> <div class="title">Spline-based RRT path planner for non-holonomic robots</div> <div class="author"> Kwangjin Yang, Sangwoo Moon, Seunghoon Yoo, <em>Jaehyeon Kang</em>, Nakju Doh, Hong Bong Kim, and Sanghyun Joo </div> <div class="periodical"> <em>Journal of Intelligent &amp; Robotic Systems</em>, 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1007/s10846-013-9963-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Planning in a cluttered environment under differential constraints is a difficult problem because the planner must satisfy the external constraints that arise from obstacles in the environment and the internal constraints due to the kinematic/dynamic limitations of the robot. This paper proposes a novel Spline-based Rapidly-exploring Random Tree (SRRT) algorithm which treats both the external and internal constraints simultaneously and efficiently. The computationally expensive numerical integration of the system dynamics is replaced by an efficient spline curve parameterization. In addition, the SRRT guarantees continuity of curvature along the path satisfying any upper-bounded curvature constraints. This paper presents the underlying theory to the SRRT algorithm and presents simulation and experiment results of a mobile robot efficiently navigating through cluttered environments.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Jaehyeon Kang. Last updated: January 30, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>